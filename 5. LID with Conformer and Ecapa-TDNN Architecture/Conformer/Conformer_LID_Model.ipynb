{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Somie12/Speech-Synthesis-for-Low-Resource-Language/blob/main/5.%20LID%20with%20Conformer%20and%20Ecapa-TDNN%20Architecture/Conformer/Conformer_LID_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aSrBSuxTGoRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSD_UquJL7Es",
        "outputId": "6645add1-838f-430c-bce8-540e0fb6ffca"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baEu8CmHTViZ",
        "outputId": "6d4afd62-57c3-4adf-d87e-a530aa1db12a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (0.13.1)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m123.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m108.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "# Installing required packages\n",
        "!pip install torch torchaudio transformers datasets soundfile librosa numpy pandas matplotlib seaborn\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import shutil\n",
        "import zipfile\n",
        "import logging\n",
        "import torch\n",
        "import torchaudio\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "IDJNK84CjsnL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining paths\n",
        "DRIVE_PATH = '/content/drive/MyDrive'\n",
        "DATASET_DRIVE_PATH = os.path.join(DRIVE_PATH, 'LID_Training', 'train_datasets')\n",
        "OUTPUT_PATH = '/content'  # Using Colab's temporary storage instead of Drive\n",
        "\n",
        "# Creating directories for extracted data in Colab space\n",
        "EXTRACTED_DATA_PATH = os.path.join(OUTPUT_PATH, 'language_data')\n",
        "os.makedirs(EXTRACTED_DATA_PATH, exist_ok=True)\n",
        "\n",
        "# Creating directory for final processed data and model checkpoints in Drive\n",
        "PROCESSED_DATA_DIR = os.path.join(DRIVE_PATH, 'LID_Training', 'processed_data')\n",
        "MODEL_CHECKPOINT_DIR = os.path.join(DRIVE_PATH, 'LID_Training', 'model_checkpoints')\n",
        "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
        "os.makedirs(MODEL_CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "# Extracting dataset zip files to Colab space\n",
        "def extract_datasets():\n",
        "    languages = ['Hindi', 'English', 'Chinese']\n",
        "    for lang in languages:\n",
        "        zip_path = os.path.join(DATASET_DRIVE_PATH, f'{lang}_Datasets.zip')\n",
        "        if os.path.exists(zip_path):\n",
        "            extract_dir = os.path.join(EXTRACTED_DATA_PATH, lang.lower())\n",
        "            os.makedirs(extract_dir, exist_ok=True)\n",
        "            print(f\"Extracting {lang} dataset to Colab space...\")\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(extract_dir)\n",
        "            print(f\"{lang} dataset extracted to {extract_dir}\")\n",
        "        else:\n",
        "            print(f\"Warning: {zip_path} not found!\")\n",
        "\n",
        "\n",
        "extract_datasets()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-9GKQXvT5Yi",
        "outputId": "0a552b1e-d8ce-4b85-dbb1-61e8523a4a64"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Hindi dataset to Colab space...\n",
            "Hindi dataset extracted to /content/language_data/hindi\n",
            "Extracting English dataset to Colab space...\n",
            "English dataset extracted to /content/language_data/english\n",
            "Extracting Chinese dataset to Colab space...\n",
            "Chinese dataset extracted to /content/language_data/chinese\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up logging\n",
        "def setup_logging(log_dir):\n",
        "    \"\"\"Setup logging configuration\"\"\"\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "    log_file = os.path.join(log_dir, f'training_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log')\n",
        "\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "        handlers=[\n",
        "            logging.FileHandler(log_file),\n",
        "            logging.StreamHandler()\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return logging.getLogger('LID_Training')"
      ],
      "metadata": {
        "id": "uX02oSJjUh4y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Voice Activity Detection (VAD) function for detecting speech segments\n",
        "def apply_vad(waveform, sample_rate=16000, threshold_db=-35, min_silence_duration_ms=300):\n",
        "    \"\"\"\n",
        "    Applying Voice Activity Detection to remove silence from audio\n",
        "\n",
        "    Args:\n",
        "        waveform: Audio signal\n",
        "        sample_rate: Audio sample rate\n",
        "        threshold_db: Energy threshold in dB below which frames are considered silence\n",
        "        min_silence_duration_ms: Minimum silence duration in milliseconds\n",
        "\n",
        "    Returns:\n",
        "        Filtered audio with silence removed\n",
        "    \"\"\"\n",
        "    # Converting to mono if stereo\n",
        "    if len(waveform.shape) > 1 and waveform.shape[0] > 1:\n",
        "        waveform = np.mean(waveform, axis=0)\n",
        "\n",
        "    # Calculating frame length and hop length\n",
        "    frame_length = int(sample_rate * 0.025)  # 25ms frames\n",
        "    hop_length = int(sample_rate * 0.010)    # 10ms hop\n",
        "\n",
        "    # Calculating energy in dB for each frame\n",
        "    energy = librosa.feature.rms(y=waveform, frame_length=frame_length, hop_length=hop_length)[0]\n",
        "    energy_db = librosa.amplitude_to_db(energy, ref=np.max)\n",
        "\n",
        "    # Creating mask for frames with energy above threshold\n",
        "    mask = energy_db > threshold_db\n",
        "\n",
        "    # Converting frame-level mask to sample-level mask\n",
        "    sample_mask = np.zeros_like(waveform, dtype=bool)\n",
        "    for i, val in enumerate(mask):\n",
        "        if val:\n",
        "            start_sample = i * hop_length\n",
        "            end_sample = min(start_sample + frame_length, len(waveform))\n",
        "            sample_mask[start_sample:end_sample] = True\n",
        "\n",
        "    # Applying minimum silence duration constraint\n",
        "    min_silence_samples = int(min_silence_duration_ms * sample_rate / 1000)\n",
        "\n",
        "    # Finding silence segments\n",
        "    silence_starts = np.where(np.diff(sample_mask.astype(int)) == -1)[0] + 1\n",
        "    silence_ends = np.where(np.diff(sample_mask.astype(int)) == 1)[0] + 1\n",
        "\n",
        "    # Handling case where audio starts with silence\n",
        "    if not sample_mask[0]:\n",
        "        silence_starts = np.insert(silence_starts, 0, 0)\n",
        "\n",
        "    # Handling case where audio ends with silence\n",
        "    if not sample_mask[-1]:\n",
        "        silence_ends = np.append(silence_ends, len(sample_mask))\n",
        "\n",
        "    # Keeping only speech segments (inverse of silence)\n",
        "    speech_segments = []\n",
        "    last_end = 0\n",
        "\n",
        "    for start, end in zip(silence_starts, silence_ends):\n",
        "        # If silence duration is less than threshold, we'll consider it as speech\n",
        "        if end - start < min_silence_samples:\n",
        "            continue\n",
        "\n",
        "        if start > last_end:\n",
        "            speech_segments.append(waveform[last_end:start])\n",
        "\n",
        "        last_end = end\n",
        "\n",
        "    # Adding remaining speech at the end if any\n",
        "    if last_end < len(waveform):\n",
        "        speech_segments.append(waveform[last_end:])\n",
        "\n",
        "    # If no speech detected, return original\n",
        "    if not speech_segments:\n",
        "        return waveform\n",
        "\n",
        "    # Concatenating all speech segments\n",
        "    return np.concatenate(speech_segments)\n",
        "\n",
        "class LanguageAudioDataset(Dataset):\n",
        "    def __init__(self, root_dir, languages=['hindi', 'english', 'chinese'], max_samples_per_lang=15000, segment_length=3,\n",
        "                 apply_vad=True):\n",
        "        \"\"\"\n",
        "        Dataset for language identification from audio files\n",
        "\n",
        "        Args:\n",
        "            root_dir: Root directory containing language folders\n",
        "            languages: List of language names (folder names)\n",
        "            max_samples_per_lang: Maximum number of samples per language\n",
        "            segment_length: Length of audio segments in seconds\n",
        "            apply_vad: Whether to apply Voice Activity Detection\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.languages = languages\n",
        "        self.segment_length = segment_length  # in seconds\n",
        "        self.sample_rate = 16000  # standard sample rate\n",
        "        self.samples = []\n",
        "        self.labels = []\n",
        "        self.apply_vad = apply_vad\n",
        "\n",
        "        for i, lang in enumerate(languages):\n",
        "            lang_dir = os.path.join(root_dir, lang)\n",
        "            if not os.path.exists(lang_dir):\n",
        "                print(f\"Warning: Directory {lang_dir} not found!\")\n",
        "                continue\n",
        "\n",
        "            audio_files = []\n",
        "            for root, _, files in os.walk(lang_dir):\n",
        "                for file in files:\n",
        "                    if file.endswith(('.wav', '.mp3', '.flac')):\n",
        "                        audio_files.append(os.path.join(root, file))\n",
        "\n",
        "            # Limiting the number of samples per language\n",
        "            audio_files = audio_files[:max_samples_per_lang]\n",
        "            print(f\"Found {len(audio_files)} files for {lang}\")\n",
        "\n",
        "            for audio_file in audio_files:\n",
        "                self.samples.append(audio_file)\n",
        "                self.labels.append(i)\n",
        "\n",
        "        print(f\"Total samples: {len(self.samples)}\")\n",
        "        self.label_to_lang = {i: lang for i, lang in enumerate(languages)}\n",
        "        self.lang_to_label = {lang: i for i, lang in enumerate(languages)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path = self.samples[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "\n",
        "        try:\n",
        "            waveform, sr = librosa.load(audio_path, sr=self.sample_rate, mono=True)\n",
        "\n",
        "            # Applying Voice Activity Detection if enabled\n",
        "            if self.apply_vad:\n",
        "                waveform = apply_vad(waveform, sample_rate=self.sample_rate)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {audio_path}: {e}\")\n",
        "            # Returns a zero array if file can't be loaded\n",
        "            return torch.zeros(self.segment_length * self.sample_rate), label\n",
        "\n",
        "        # Processes audio into 3-second segments\n",
        "        segments = self._segment_audio(waveform)\n",
        "\n",
        "        # If no valid segments, creates a zero segment\n",
        "        if len(segments) == 0:\n",
        "            return torch.zeros(self.segment_length * self.sample_rate), label\n",
        "\n",
        "        # Returns random segment (during training) or first segment (during validation)\n",
        "        segment_idx = np.random.randint(0, len(segments)) if len(segments) > 1 else 0\n",
        "        return torch.from_numpy(segments[segment_idx].astype(np.float32)), label\n",
        "\n",
        "    def _segment_audio(self, waveform):\n",
        "        \"\"\"Split audio into 3-second segments, discard extra seconds\"\"\"\n",
        "        segment_samples = self.segment_length * self.sample_rate\n",
        "        segments = []\n",
        "\n",
        "        # If audio is shorter than 3 seconds, skip it\n",
        "        if len(waveform) < segment_samples:\n",
        "            return segments\n",
        "\n",
        "        # Split longer audio into 3-second segments\n",
        "        for i in range(0, len(waveform) - segment_samples + 1, segment_samples):\n",
        "            segment = waveform[i:i + segment_samples]\n",
        "            segments.append(segment)\n",
        "\n",
        "        return segments"
      ],
      "metadata": {
        "id": "B9vNGpsSViNk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Language Identification using Conformer Architecture (PyTorch)\n",
        "\n",
        "This model processes raw audio or log-mel spectrograms to classify the spoken language.\n",
        "Key modules:\n",
        "- extract_features: Converts waveform to normalized log-mel spectrogram.\n",
        "- ConformerBlock: Combines self-attention, convolution, and feedforward layers.\n",
        "- Conformer: Stacked blocks with positional encoding and classification head.\n",
        "- LIDModel: Full pipeline for LID, handling raw audio input and prediction.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
        "\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.linear_layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(3)])\n",
        "        self.output_linear = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # Linear projections and split into h heads\n",
        "        query, key, value = [\n",
        "            l(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
        "            for l, x in zip(self.linear_layers, (query, key, value))\n",
        "        ]\n",
        "\n",
        "        # Apply attention on all projected vectors in batch\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        p_attn = F.softmax(scores, dim=-1)\n",
        "        p_attn = self.dropout(p_attn)\n",
        "        x = torch.matmul(p_attn, value)\n",
        "\n",
        "        # Combine heads\n",
        "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)\n",
        "\n",
        "        return self.output_linear(x)\n",
        "\n",
        "class ConvModule(nn.Module):\n",
        "    def __init__(self, d_model, kernel_size=31, dropout=0.1):\n",
        "        super(ConvModule, self).__init__()\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Pointwise convolution\n",
        "        self.pointwise_conv1 = nn.Conv1d(d_model, d_model * 2, kernel_size=1)\n",
        "\n",
        "        # 1D depthwise convolution\n",
        "        padding = (kernel_size - 1) // 2\n",
        "        self.depthwise_conv = nn.Conv1d(\n",
        "            d_model, d_model, kernel_size=kernel_size, padding=padding, groups=d_model\n",
        "        )\n",
        "\n",
        "        self.batch_norm = nn.BatchNorm1d(d_model)\n",
        "        self.activation = nn.SiLU()  # SiLU (Swish) activation\n",
        "\n",
        "        # Pointwise convolution\n",
        "        self.pointwise_conv2 = nn.Conv1d(d_model, d_model, kernel_size=1)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch_size, seq_len, d_model]\n",
        "        residual = x\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        # Transpose for conv operations\n",
        "        x = x.transpose(1, 2)  # [batch_size, d_model, seq_len]\n",
        "\n",
        "        # GLU mechanism\n",
        "        x = self.pointwise_conv1(x)\n",
        "        x = F.glu(x, dim=1)  # Dimension halved here\n",
        "\n",
        "        # Depthwise convolution\n",
        "        x = self.depthwise_conv(x)\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        # Second pointwise convolution\n",
        "        x = self.pointwise_conv2(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Transpose back\n",
        "        x = x.transpose(1, 2)  # [batch_size, seq_len, d_model]\n",
        "\n",
        "        # Residual connection\n",
        "        return x + residual\n",
        "\n",
        "class ConformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, heads, kernel_size, dropout=0.1):\n",
        "        super(ConformerBlock, self).__init__()\n",
        "\n",
        "        self.ff1 = FeedForward(d_model, d_ff, dropout)\n",
        "        self.ff1_factor = 0.5\n",
        "\n",
        "        self.self_attn = MultiHeadedAttention(heads, d_model, dropout)\n",
        "        self.attn_layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.conv_module = ConvModule(d_model, kernel_size, dropout)\n",
        "\n",
        "        self.ff2 = FeedForward(d_model, d_ff, dropout)\n",
        "        self.ff2_factor = 0.5\n",
        "\n",
        "        self.final_layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # First Feed Forward module\n",
        "        x = x + self.ff1_factor * self.ff1(x)\n",
        "\n",
        "        # Multi-Headed Self-Attention\n",
        "        residual = x\n",
        "        x = self.attn_layer_norm(x)\n",
        "        x = residual + self.self_attn(x, x, x, mask)\n",
        "\n",
        "        # Convolution module\n",
        "        x = self.conv_module(x)\n",
        "\n",
        "        # Second Feed Forward module\n",
        "        x = x + self.ff2_factor * self.ff2(x)\n",
        "\n",
        "        # Final Layer Norm\n",
        "        x = self.final_layer_norm(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Conformer(nn.Module):\n",
        "    def __init__(self, num_classes, d_model=144, n_layers=6, n_heads=4, d_ff=256,\n",
        "                 kernel_size=31, dropout=0.1, input_dim=80):\n",
        "        super(Conformer, self).__init__()\n",
        "\n",
        "        # Input projection from mel spectrogram to d_model dimension\n",
        "        self.input_projection = nn.Linear(input_dim, d_model)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.positional_encoding = PositionalEncoding(d_model)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Conformer blocks\n",
        "        self.layers = nn.ModuleList([\n",
        "            ConformerBlock(d_model, d_ff, n_heads, kernel_size, dropout)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # x shape: [batch_size, channels, time, freq]\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Convert to [batch_size, time, freq] if needed\n",
        "        if x.dim() == 4:\n",
        "            x = x.squeeze(1)  # Remove channel dimension if present\n",
        "\n",
        "        # Reshape input to [batch_size, seq_len, input_dim]\n",
        "        # Assuming input is [batch_size, n_mels, time] from extract_features\n",
        "        x = x.transpose(1, 2) # [batch_size, time, n_mels]\n",
        "\n",
        "        # Project input to d_model dimension\n",
        "        x = self.input_projection(x)\n",
        "\n",
        "        # Add positional encoding\n",
        "        x = self.positional_encoding(x)\n",
        "\n",
        "        # Apply dropout\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Apply Conformer blocks\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "\n",
        "        # Global average pooling\n",
        "        x = torch.mean(x, dim=1)\n",
        "\n",
        "        # Classification\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class LIDModel(nn.Module):\n",
        "   def __init__(self, num_languages=3, input_dim=80):\n",
        "        super(LIDModel, self).__init__()\n",
        "\n",
        "        # Conformer-based encoder\n",
        "        self.conformer = Conformer(\n",
        "            num_classes=num_languages,\n",
        "            d_model=144,  # Dimension of model\n",
        "            n_layers=6,   # Number of Conformer blocks\n",
        "            n_heads=4,    # Number of attention heads\n",
        "            d_ff=256,     # Feed forward dimension\n",
        "            kernel_size=31,  # Kernel size for convolution module\n",
        "            dropout=0.1,  # Dropout rate\n",
        "            input_dim=input_dim  # Input dimension (mel spectrogram features)\n",
        "        )\n",
        "\n",
        "   def forward(self, x):\n",
        "        # Extract features if input is raw audio\n",
        "        if x.dim() == 2:\n",
        "            # Input is [batch_size, time]\n",
        "            # Convert to mel spectrogram features\n",
        "            # Features will be [batch_size, n_mels, time]\n",
        "            # Ensure features are on the same device as input x\n",
        "            x = extract_features(x)\n",
        "            # The Conformer expects [batch_size, time, input_dim] after reshape/projection\n",
        "            # The current extract_features returns [batch_size, n_mels, time]\n",
        "            # This reshape is handled inside the Conformer's forward method\n",
        "\n",
        "        # Forward through conformer\n",
        "        return self.conformer(x)"
      ],
      "metadata": {
        "id": "EI1YYviBWQ05"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function  extract features (mel spectrograms) from audio\n",
        "def extract_features(waveform, sample_rate=16000, n_mels=80, device='cpu'): # Add device argument\n",
        "    \"\"\"\n",
        "    Convert waveform to mel spectrogram for model input\n",
        "\n",
        "    Args:\n",
        "        waveform: Audio waveform (can be on CPU or GPU)\n",
        "        sample_rate: Audio sample rate\n",
        "        n_mels: Number of mel bands\n",
        "        device: The device the waveform is on\n",
        "\n",
        "    Returns:\n",
        "        Normalized log mel spectrogram\n",
        "    \"\"\"\n",
        "    # Ensure the transform is on the same device as the waveform\n",
        "    mel_spectrogram_transform = torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=sample_rate,\n",
        "        n_fft=512,\n",
        "        win_length=400,\n",
        "        hop_length=160,  # 10ms shift\n",
        "        n_mels=n_mels\n",
        "    ).to(device) # Move the transform to the specified device\n",
        "\n",
        "    mel_spec = mel_spectrogram_transform(waveform)\n",
        "\n",
        "    # Convert to log mel spectrogram\n",
        "    log_mel = torch.log(mel_spec + 1e-9)\n",
        "\n",
        "    # Normalize\n",
        "    mean = log_mel.mean()\n",
        "    std = log_mel.std()\n",
        "    log_mel = (log_mel - mean) / (std + 1e-9)\n",
        "\n",
        "    return log_mel"
      ],
      "metadata": {
        "id": "HJd2l8veWUtE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for training the model with enhanced logging\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
        "                num_epochs=30, device='cuda', save_dir=None, logger=None,\n",
        "                start_epoch=0, resume_training=False):\n",
        "    \"\"\"\n",
        "    training function with logging and metrics tracking\n",
        "\n",
        "    Args:\n",
        "        model: Model to train\n",
        "        train_loader: DataLoader for training set\n",
        "        val_loader: DataLoader for validation set\n",
        "        criterion: Loss function\n",
        "        optimizer: Optimizer\n",
        "        scheduler: Learning rate scheduler\n",
        "        num_epochs: Number of epochs to train\n",
        "        device: Device to train on\n",
        "        save_dir: Directory to save checkpoints and logs\n",
        "        logger: Logger instance for detailed logging\n",
        "        start_epoch: Epoch to start from (for resuming training)\n",
        "        resume_training: Whether this is a resumed training session\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing training history\n",
        "    \"\"\"\n",
        "    if logger is None:\n",
        "        logger = logging.getLogger('LID_Training')\n",
        "\n",
        "    # Creating directories for saving various outputs\n",
        "    os.makedirs(os.path.join(save_dir, 'checkpoints'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(save_dir, 'logs'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(save_dir, 'visualizations'), exist_ok=True)\n",
        "    # Create directory for per-epoch models\n",
        "    os.makedirs(os.path.join(save_dir, 'epoch_models'), exist_ok=True)\n",
        "\n",
        "    # Keeping track of metrics\n",
        "    best_val_acc = 0.0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "\n",
        "    # For detailed per-epoch logging\n",
        "    epoch_metrics = []\n",
        "\n",
        "    # For resuming training\n",
        "    if resume_training and start_epoch > 0:\n",
        "        history_path = os.path.join(save_dir, 'logs', 'training_history.json')\n",
        "        if os.path.exists(history_path):\n",
        "            try:\n",
        "                with open(history_path, 'r') as f:\n",
        "                    history_data = json.load(f)\n",
        "                    train_losses = history_data['train_loss'][:start_epoch]\n",
        "                    val_losses = history_data['val_loss'][:start_epoch]\n",
        "                    train_accs = history_data['train_acc'][:start_epoch]\n",
        "                    val_accs = history_data['val_acc'][:start_epoch]\n",
        "                    epoch_metrics = history_data['epoch_metrics'][:start_epoch]\n",
        "\n",
        "                    # Finding the best validation accuracy so far\n",
        "                    for metric in epoch_metrics:\n",
        "                        if metric['val_acc'] > best_val_acc:\n",
        "                            best_val_acc = metric['val_acc']\n",
        "\n",
        "                    logger.info(f\"Loaded training history up to epoch {start_epoch}, best val acc: {best_val_acc:.2f}%\")\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Failed to load training history: {e}\")\n",
        "\n",
        "    # Training start time\n",
        "    start_time = time.time()\n",
        "    logger.info(f\"Starting training for {num_epochs} epochs (from epoch {start_epoch+1})\")\n",
        "\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        batch_losses = []\n",
        "\n",
        "        logger.info(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        # Creating progress bar for training\n",
        "        train_pbar = tqdm(train_loader, desc=f\"Train Epoch {epoch+1}\")\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_pbar):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Converting inputs to spectrograms if needed\n",
        "            if inputs.dim() == 2:  # [batch, time]\n",
        "                specs = []\n",
        "                for waveform in inputs:\n",
        "                    # Pass the device to extract_features\n",
        "                    spec = extract_features(waveform, device=device)\n",
        "                    specs.append(spec)\n",
        "                inputs = torch.stack(specs)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Statistics\n",
        "            train_loss += loss.item()\n",
        "            batch_losses.append(loss.item())\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            # Updating progress bar\n",
        "            train_pbar.set_postfix({\n",
        "                'loss': f\"{loss.item():.4f}\",\n",
        "                'acc': f\"{100.*correct/total:.2f}%\"\n",
        "            })\n",
        "\n",
        "        # Calculating training metrics\n",
        "        train_acc = 100. * correct / total\n",
        "        train_loss /= len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        # Creates progress bar for validation\n",
        "        val_pbar = tqdm(val_loader, desc=f\"Val Epoch {epoch+1}\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_pbar:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "                # Converting inputs to spectrograms if needed\n",
        "                if inputs.dim() == 2:  # [batch, time]\n",
        "                    specs = []\n",
        "                    for waveform in inputs:\n",
        "                        # Pass the device to extract_features\n",
        "                        spec = extract_features(waveform, device=device)\n",
        "                        specs.append(spec)\n",
        "                    inputs = torch.stack(specs)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += targets.size(0)\n",
        "                correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "                # Updating progress bar\n",
        "                val_pbar.set_postfix({\n",
        "                    'loss': f\"{loss.item():.4f}\",\n",
        "                    'acc': f\"{100.*correct/total:.2f}%\"\n",
        "                })\n",
        "\n",
        "        # Calculates validation metrics\n",
        "        val_acc = 100. * correct / total\n",
        "        val_loss /= len(val_loader)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        # Calculates epoch time\n",
        "        epoch_time = time.time() - epoch_start_time\n",
        "\n",
        "        # Log epoch results\n",
        "        logger.info(f\"Epoch {epoch+1} completed in {epoch_time:.2f}s - \"\n",
        "                   f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n",
        "                   f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # Saving epoch metrics\n",
        "        epoch_metric = {\n",
        "            'epoch': epoch + 1,\n",
        "            'train_loss': train_loss,\n",
        "            'train_acc': train_acc,\n",
        "            'val_loss': val_loss,\n",
        "            'val_acc': val_acc,\n",
        "            'learning_rate': optimizer.param_groups[0]['lr'],\n",
        "            'epoch_time': epoch_time,\n",
        "            'batch_losses': batch_losses  # Saving all batch losses for distribution analysis\n",
        "        }\n",
        "        epoch_metrics.append(epoch_metric)\n",
        "\n",
        "        # Saving metrics to CSV after each epoch\n",
        "        metrics_df = pd.DataFrame(epoch_metrics)\n",
        "        metrics_df.to_csv(os.path.join(save_dir, 'logs', 'training_metrics.csv'), index=False)\n",
        "\n",
        "        # Plotting and saving learning curves after each epoch\n",
        "        plot_learning_curves(train_losses, val_losses, train_accs, val_accs,\n",
        "                            save_path=os.path.join(save_dir, 'visualizations', f'learning_curves_epoch_{epoch+1}.png'))\n",
        "\n",
        "        # Plotting batch loss distribution\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.hist(batch_losses, bins=30, alpha=0.7)\n",
        "        plt.xlabel('Batch Loss')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.title(f'Batch Loss Distribution - Epoch {epoch+1}')\n",
        "        plt.savefig(os.path.join(save_dir, 'visualizations', f'batch_loss_dist_epoch_{epoch+1}.png'))\n",
        "        plt.close()\n",
        "\n",
        "        # Updating the learning rate\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Save model after each epoch in the epoch_models folder\n",
        "        epoch_model_filename = f'model_epoch_{epoch+1}_trainloss_{train_loss:.4f}_trainacc_{train_acc:.2f}_valloss_{val_loss:.4f}_valacc_{val_acc:.2f}.pt'\n",
        "        epoch_model_path = os.path.join(save_dir, 'epoch_models', epoch_model_filename)\n",
        "\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'val_acc': val_acc,\n",
        "            'val_loss': val_loss,\n",
        "            'train_acc': train_acc,\n",
        "            'train_loss': train_loss,\n",
        "            'train_losses': train_losses,\n",
        "            'val_losses': val_losses,\n",
        "            'train_accs': train_accs,\n",
        "            'val_accs': val_accs,\n",
        "            'epoch_metrics': epoch_metrics\n",
        "        }, epoch_model_path)\n",
        "\n",
        "        logger.info(f\"Saved epoch model to {epoch_model_path}\")\n",
        "\n",
        "        # Saving the model checkpoint if it's the best so far\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            checkpoint_path = os.path.join(save_dir, 'checkpoints', f'best_model_epoch_{epoch+1}.pt')\n",
        "            torch.save({\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'val_acc': val_acc,\n",
        "                'val_loss': val_loss,\n",
        "                'train_acc': train_acc,\n",
        "                'train_loss': train_loss,\n",
        "            }, checkpoint_path)\n",
        "            logger.info(f\"New best model saved to {checkpoint_path} with validation accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "        # Saving regular checkpoint every 5 epochs\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            checkpoint_path = os.path.join(save_dir, 'checkpoints', f'model_epoch_{epoch+1}.pt')\n",
        "            torch.save({\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'val_acc': val_acc,\n",
        "                'val_loss': val_loss,\n",
        "                'train_acc': train_acc,\n",
        "                'train_loss': train_loss,\n",
        "            }, checkpoint_path)\n",
        "            logger.info(f\"Regular checkpoint saved to {checkpoint_path}\")\n",
        "\n",
        "    # Calculating total training time\n",
        "    total_time = time.time() - start_time\n",
        "    logger.info(f\"Training completed in {total_time/60:.2f} minutes\")\n",
        "\n",
        "    # Saving the final model\n",
        "    final_path = os.path.join(save_dir, 'checkpoints', 'final_model.pt')\n",
        "    torch.save({\n",
        "        'epoch': num_epochs,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'val_acc': val_acc,\n",
        "        'val_loss': val_loss,\n",
        "        'train_acc': train_acc,\n",
        "        'train_loss': train_loss,\n",
        "    }, final_path)\n",
        "    logger.info(f\"Final model saved to {final_path}\")\n",
        "\n",
        "    # Creating and saving final visualizations\n",
        "    create_final_visualizations(train_losses, val_losses, train_accs, val_accs, epoch_metrics, save_dir)\n",
        "\n",
        "    # Returning training history\n",
        "    history = {\n",
        "        'train_loss': train_losses,\n",
        "        'val_loss': val_losses,\n",
        "        'train_acc': train_accs,\n",
        "        'val_acc': val_accs,\n",
        "        'epoch_metrics': epoch_metrics\n",
        "    }\n",
        "\n",
        "    # Saving history as JSON\n",
        "    with open(os.path.join(save_dir, 'logs', 'training_history.json'), 'w') as f:\n",
        "        # Converting numpy values to Python native types for JSON serialization\n",
        "        history_json = {\n",
        "            'train_loss': [float(x) for x in train_losses],\n",
        "            'val_loss': [float(x) for x in val_losses],\n",
        "            'train_acc': [float(x) for x in train_accs],\n",
        "            'val_acc': [float(x) for x in val_accs],\n",
        "            'epoch_metrics': [{k: float(v) if isinstance(v, (np.float32, np.float64)) and k != 'batch_losses' else\n",
        "                               [float(x) for x in v] if k == 'batch_losses' else v\n",
        "                               for k, v in m.items()} for m in epoch_metrics]\n",
        "        }\n",
        "        json.dump(history_json, f, indent=4)\n",
        "\n",
        "    return history\n"
      ],
      "metadata": {
        "id": "b6tUo9BSWlil"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for resuming training from a checkpoint\n",
        "def resume_training(checkpoint_path, model, optimizer, scheduler, logger):\n",
        "    \"\"\"\n",
        "    Resumes training from a checkpoint\n",
        "\n",
        "    Args:\n",
        "        checkpoint_path: Path to the checkpoint file\n",
        "        model: Model to load weights into\n",
        "        optimizer: Optimizer to load state into\n",
        "        scheduler: Learning rate scheduler to load state into\n",
        "        logger: Logger instance\n",
        "\n",
        "    Returns:\n",
        "        start_epoch: Epoch to start from\n",
        "        model: Loaded model\n",
        "        optimizer: Loaded optimizer\n",
        "        scheduler: Loaded scheduler\n",
        "    \"\"\"\n",
        "    logger.info(f\"Loading checkpoint from {checkpoint_path}\")\n",
        "\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    start_epoch = checkpoint['epoch']\n",
        "\n",
        "    logger.info(f\"Checkpoint loaded successfully. Resuming from epoch {start_epoch}\")\n",
        "    logger.info(f\"Loaded model with val_acc: {checkpoint['val_acc']:.2f}%, val_loss: {checkpoint['val_loss']:.4f}\")\n",
        "\n",
        "    return start_epoch, model, optimizer, scheduler\n",
        "\n"
      ],
      "metadata": {
        "id": "Mv8ur2eJ1_nS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_learning_curves(train_losses, val_losses, train_accs, val_accs, save_path=None):\n",
        "    \"\"\"Plot learning curves for loss and accuracy\"\"\"\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accs, label='Train Accuracy')\n",
        "    plt.plot(val_accs, label='Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "\n",
        "    plt.close()\n",
        "\n",
        "def create_final_visualizations(train_losses, val_losses, train_accs, val_accs, epoch_metrics, save_dir):\n",
        "    \"\"\"Creating visualizations for the entire training process\"\"\"\n",
        "    vis_dir = os.path.join(save_dir, 'visualizations')\n",
        "    os.makedirs(vis_dir, exist_ok=True)\n",
        "\n",
        "    # 1. Learning curves\n",
        "    plot_learning_curves(train_losses, val_losses, train_accs, val_accs,\n",
        "                         save_path=os.path.join(vis_dir, 'final_learning_curves.png'))\n",
        "\n",
        "    # 2. Learning rate schedule\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    learning_rates = [m['learning_rate'] for m in epoch_metrics]\n",
        "    plt.plot(range(1, len(learning_rates) + 1), learning_rates, marker='o')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Learning Rate')\n",
        "    plt.title('Learning Rate Schedule')\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.savefig(os.path.join(vis_dir, 'learning_rate_schedule.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # 3. Epoch training time\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    epoch_times = [m['epoch_time'] for m in epoch_metrics]\n",
        "    plt.plot(range(1, len(epoch_times) + 1), epoch_times, marker='o')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Training Time (seconds)')\n",
        "    plt.title('Epoch Training Time')\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.savefig(os.path.join(vis_dir, 'epoch_training_times.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # 4. Combined metrics plot\n",
        "    plt.figure(figsize=(12, 10))\n",
        "\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Loss Curves')\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(train_accs, label='Train Accuracy')\n",
        "    plt.plot(val_accs, label='Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    plt.title('Accuracy Curves')\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(learning_rates, marker='o')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Learning Rate')\n",
        "    plt.title('Learning Rate Schedule')\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(epoch_times, marker='o')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Time (seconds)')\n",
        "    plt.title('Epoch Training Time')\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(vis_dir, 'combined_training_metrics.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # 5. Correlation heatmap\n",
        "    metrics_df = pd.DataFrame(epoch_metrics)\n",
        "    corr_columns = ['train_loss', 'val_loss', 'train_acc', 'val_acc', 'learning_rate', 'epoch_time']\n",
        "    corr_df = metrics_df[corr_columns].corr()\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(corr_df, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
        "    plt.title('Correlation Between Training Metrics')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(vis_dir, 'metrics_correlation.png'))\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "CzNAXCldXPrb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, test_loader, criterion, device, languages, save_dir=None, logger=None):\n",
        "    \"\"\"\n",
        "     Testing function that evaluates model performance\n",
        "    and generates visualizations\n",
        "\n",
        "    Args:\n",
        "        model: Trained model\n",
        "        test_loader: DataLoader for test dataset\n",
        "        criterion: Loss function\n",
        "        device: Device to run evaluation on\n",
        "        languages: List of language names\n",
        "        save_dir: Directory to save visualizations\n",
        "        logger: Logger instance\n",
        "\n",
        "    Returns:\n",
        "        test_loss: Average loss on test set\n",
        "        test_acc: Accuracy on test set\n",
        "        per_class_metrics: Dictionary of per-class metrics\n",
        "    \"\"\"\n",
        "    if logger is None:\n",
        "        logger = logging.getLogger('LID_Training')\n",
        "\n",
        "    # Creating directory for test results\n",
        "    test_dir = os.path.join(save_dir, 'test_results')\n",
        "    os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "    logger.info(\"Starting evaluation on test set\")\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # For per-class metrics\n",
        "    num_classes = len(languages)\n",
        "    class_correct = list(0. for i in range(num_classes))\n",
        "    class_total = list(0. for i in range(num_classes))\n",
        "\n",
        "    # For confusion matrix\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    # For ROC curve\n",
        "    all_scores = []\n",
        "\n",
        "    # For sample-level analysis\n",
        "    test_samples = []\n",
        "\n",
        "    test_start_time = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in tqdm(test_loader, desc=\"Testing\"):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Converting inputs to spectrograms if needed\n",
        "            if inputs.dim() == 2:  # [batch, time]\n",
        "                specs = []\n",
        "                for waveform in inputs:\n",
        "                     # Pass the device to extract_features\n",
        "                    spec = extract_features(waveform, device=device)\n",
        "                    specs.append(spec)\n",
        "                inputs = torch.stack(specs)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Collectting statistics\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            # Per-class accuracy\n",
        "            c = (predicted == targets).squeeze()\n",
        "            # Check if c is a single boolean value (batch size 1) or a tensor\n",
        "            if c.ndim == 0:\n",
        "                 if targets.size(0) > 0: # Ensure batch is not empty\n",
        "                     label = targets[0].item()\n",
        "                     class_correct[label] += int(c.item()) # Convert boolean tensor to int\n",
        "                     class_total[label] += 1\n",
        "            else: # c is a tensor for batch size > 1\n",
        "                for i in range(targets.size(0)):\n",
        "                    label = targets[i].item()\n",
        "                    class_correct[label] += c[i].item()\n",
        "                    class_total[label] += 1\n",
        "\n",
        "            # Collect predictions and targets for confusion matrix\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "            # Collect scores for ROC curve\n",
        "            probs = F.softmax(outputs, dim=1).cpu().numpy()\n",
        "            all_scores.extend(probs)\n",
        "\n",
        "            # Sample-level analysis\n",
        "            for i in range(len(targets)):\n",
        "                sample_info = {\n",
        "                    'true_label': int(targets[i].item()),\n",
        "                    'pred_label': int(predicted[i].item()),\n",
        "                    'correct': bool(predicted[i] == targets[i]),\n",
        "                    'probabilities': {lang: float(probs[i][j]) for j, lang in enumerate(languages)}\n",
        "                }\n",
        "                test_samples.append(sample_info)\n",
        "\n",
        "    test_time = time.time() - test_start_time\n",
        "\n",
        "    # Calculating overall metrics\n",
        "    test_loss /= len(test_loader)\n",
        "    test_acc = 100. * correct / total\n",
        "\n",
        "    logger.info(f\"Test evaluation completed in {test_time:.2f}s\")\n",
        "    logger.info(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
        "\n",
        "    # Calculating per-class metrics\n",
        "    per_class_metrics = {}\n",
        "    for i in range(num_classes):\n",
        "        if class_total[i] > 0:\n",
        "            class_acc = 100 * class_correct[i] / class_total[i]\n",
        "            logger.info(f'Accuracy of {languages[i]}: {class_acc:.2f}%')\n",
        "            per_class_metrics[languages[i]] = {\n",
        "                'accuracy': class_acc,\n",
        "                'samples': class_total[i]\n",
        "            }\n",
        "\n",
        "    # Creating and saving all test visualizations and metrics\n",
        "    if save_dir:\n",
        "        # Converting lists to numpy arrays for easier processing\n",
        "        all_targets = np.array(all_targets)\n",
        "        all_preds = np.array(all_preds)\n",
        "        all_scores = np.array(all_scores)\n",
        "\n",
        "        # 1. Confusion matrix\n",
        "        cm = confusion_matrix(all_targets, all_preds)\n",
        "        cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues',\n",
        "                    xticklabels=languages, yticklabels=languages)\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('True')\n",
        "        plt.title('Normalized Confusion Matrix')\n",
        "        plt.savefig(os.path.join(test_dir, 'confusion_matrix.png'))\n",
        "        plt.close()\n",
        "\n",
        "        # raw confusion matrix data\n",
        "        cm_df = pd.DataFrame(cm, index=languages, columns=languages)\n",
        "        cm_df.to_csv(os.path.join(test_dir, 'confusion_matrix.csv'))\n",
        "\n",
        "        # 2. Classification report with precision, recall, F1\n",
        "        precision, recall, f1, support = precision_recall_fscore_support(all_targets, all_preds)\n",
        "        metrics_df = pd.DataFrame({\n",
        "            'Language': languages,\n",
        "            'Precision': precision,\n",
        "            'Recall': recall,\n",
        "            'F1 Score': f1,\n",
        "            'Support': support\n",
        "        })\n",
        "        metrics_df.to_csv(os.path.join(test_dir, 'classification_metrics.csv'), index=False)\n",
        "\n",
        "        # 3. ROC curve and AUC\n",
        "        # Binarize labels for ROC curve\n",
        "        y_bin = label_binarize(all_targets, classes=range(len(languages)))\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        for i, language in enumerate(languages):\n",
        "            fpr, tpr, _ = roc_curve(y_bin[:, i], all_scores[:, i])\n",
        "            roc_auc = auc(fpr, tpr)\n",
        "            plt.plot(fpr, tpr, lw=2, label=f'{language} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "        plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('Receiver Operating Characteristic (ROC) Curves')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.savefig(os.path.join(test_dir, 'roc_curves.png'))\n",
        "        plt.close()\n",
        "\n",
        "        # 4. Distribution of prediction probabilities\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        for i, language in enumerate(languages):\n",
        "            # softmax scores for samples of this language\n",
        "            class_indices = np.where(all_targets == i)[0]\n",
        "            if len(class_indices) > 0:\n",
        "                correct_scores = [all_scores[j][i] for j in class_indices if all_preds[j] == i]\n",
        "                incorrect_scores = [all_scores[j][i] for j in class_indices if all_preds[j] != i]\n",
        "\n",
        "                plt.subplot(1, len(languages), i+1)\n",
        "                if correct_scores:\n",
        "                    sns.kdeplot(correct_scores, fill=True, label='Correct', alpha=0.7)\n",
        "                if incorrect_scores:\n",
        "                    sns.kdeplot(incorrect_scores, fill=True, label='Incorrect', alpha=0.7)\n",
        "                plt.title(f'{language}')\n",
        "                plt.xlabel('Confidence')\n",
        "                plt.ylabel('Density')\n",
        "                plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(test_dir, 'prediction_distributions.png'))\n",
        "        plt.close()\n",
        "\n",
        "        # 5. Sample-level analysis\n",
        "        samples_df = pd.DataFrame(test_samples)\n",
        "        samples_df.to_csv(os.path.join(test_dir, 'sample_predictions.csv'), index=False)\n",
        "\n",
        "        # 6. Misclassification analysis\n",
        "        misclassified = [(true, pred) for true, pred in zip(all_targets, all_preds) if true != pred]\n",
        "        misclass_counts = {}\n",
        "        for true, pred in misclassified:\n",
        "            pair = (languages[true], languages[pred])\n",
        "            misclass_counts[pair] = misclass_counts.get(pair, 0) + 1\n",
        "\n",
        "        misclass_df = pd.DataFrame([\n",
        "            {'True': true, 'Predicted': pred, 'Count': count}\n",
        "            for (true, pred), count in misclass_counts.items()\n",
        "        ])\n",
        "\n",
        "        if not misclass_df.empty:\n",
        "            misclass_df = misclass_df.sort_values('Count', ascending=False)\n",
        "            misclass_df.to_csv(os.path.join(test_dir, 'misclassification_analysis.csv'), index=False)\n",
        "\n",
        "            # Top misclassifications plot\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            top_n = min(10, len(misclass_df))\n",
        "            top_misclass = misclass_df.head(top_n)\n",
        "\n",
        "            sns.barplot(x='Count', y=top_misclass['True'] + ' → ' + top_misclass['Predicted'], data=top_misclass)\n",
        "            plt.title(f'Top {top_n} Misclassifications')\n",
        "            plt.xlabel('Count')\n",
        "            plt.ylabel('Misclassification')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(test_dir, 'top_misclassifications.png'))\n",
        "            plt.close()\n",
        "\n",
        "    # summary report\n",
        "    summary = {\n",
        "        'test_acc': test_acc,\n",
        "        'test_loss': test_loss,\n",
        "        'per_class_metrics': per_class_metrics,\n",
        "        'test_time': test_time,\n",
        "        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(test_dir, 'test_summary.json'), 'w') as f:\n",
        "        json.dump(summary, f, indent=4)\n",
        "\n",
        "    logger.info(f\"Test evaluation completed. Results saved to {test_dir}\")\n",
        "\n",
        "    return test_loss, test_acc, per_class_metrics"
      ],
      "metadata": {
        "id": "oqDO1aVTXuyA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for splitting dataset into train, validation, and test sets\n",
        "def split_dataset(dataset, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, seed=42):\n",
        "    \"\"\"\n",
        "    Split dataset into train, validation, and test sets\n",
        "\n",
        "    Args:\n",
        "        dataset: Dataset to split\n",
        "        train_ratio: Ratio of training set\n",
        "        val_ratio: Ratio of validation set\n",
        "        test_ratio: Ratio of test set\n",
        "        seed: Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        train_set, val_set, test_set: Split datasets\n",
        "    \"\"\"\n",
        "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-5, \"Ratios must sum to 1\"\n",
        "\n",
        "    # Setting seed for reproducibility\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Getting dataset size\n",
        "    dataset_size = len(dataset)\n",
        "\n",
        "    # Calculating split sizes\n",
        "    train_size = int(train_ratio * dataset_size)\n",
        "    val_size = int(val_ratio * dataset_size)\n",
        "    test_size = dataset_size - train_size - val_size\n",
        "\n",
        "    # Splitting dataset\n",
        "    train_set, val_set, test_set = torch.utils.data.random_split(\n",
        "        dataset, [train_size, val_size, test_size])\n",
        "\n",
        "    return train_set, val_set, test_set\n",
        "\n",
        "\n",
        "# Function for saving model as ONNX for deployment\n",
        "def save_model_as_onnx(model, save_path, input_shape=(1, 1, 80, 188)):\n",
        "    \"\"\"\n",
        "    Save PyTorch model as ONNX for deployment\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model\n",
        "        save_path: Path to save ONNX model\n",
        "        input_shape: Input shape for ONNX export\n",
        "    \"\"\"\n",
        "    # Creating dummy input\n",
        "    dummy_input = torch.randn(input_shape, requires_grad=True)\n",
        "\n",
        "    # Exporting model\n",
        "    torch.onnx.export(\n",
        "        model,                  # model being run\n",
        "        dummy_input,            # model input\n",
        "        save_path,              # where to save the model\n",
        "        export_params=True,     # storing the trained parameter weights inside the model file\n",
        "        opset_version=12,       # the ONNX version to export the model to\n",
        "        do_constant_folding=True,  # whether to execute constant folding for optimization\n",
        "        input_names=['input'],  # the model's input names\n",
        "        output_names=['output'],  # the model's output names\n",
        "        dynamic_axes={\n",
        "            'input': {0: 'batch_size'},  # variable length axes\n",
        "            'output': {0: 'batch_size'}\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print(f\"Model exported to ONNX format at {save_path}\")\n"
      ],
      "metadata": {
        "id": "tc-BmUbzYPra"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "waveform = torch.randn(1, 16000)  # Example: 1-second audio at 16kHz\n",
        "features = extract_features(waveform, sample_rate=16000, n_mels=80, device='cpu')\n",
        "print(features.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tB0hgRNV4BRo",
        "outputId": "d8b6c7d1-3707-4177-bda9-bf5d6823c831"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 80, 101])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Import torchsummary for model architecture overview\n",
        "\n",
        "\n",
        "    # Defining paths\n",
        "    DRIVE_PATH = '/content/drive/MyDrive'\n",
        "\n",
        "    # Use the correct path where the audio data was extracted\n",
        "    EXTRACTED_DATA_PATH = '/content/language_data'\n",
        "\n",
        "    print(f\"Using audio data path: {EXTRACTED_DATA_PATH}\")\n",
        "    print(f\"Checking for language directories:\")\n",
        "\n",
        "    # Verify the language directories exist\n",
        "    for lang in ['hindi', 'english', 'chinese']:\n",
        "        lang_path = os.path.join(EXTRACTED_DATA_PATH, lang)\n",
        "        if os.path.exists(lang_path):\n",
        "            print(f\"✓ Found {lang} directory with {len(os.listdir(lang_path))} items\")\n",
        "        else:\n",
        "            print(f\"✗ Missing {lang} directory!\")\n",
        "\n",
        "    SAVE_DIR = os.path.join(DRIVE_PATH, 'LID_Training', f'lid_model_results_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}')\n",
        "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "    # Setting random seeds for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    torch.cuda.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Configuration\n",
        "    config = {\n",
        "        'data_dir': EXTRACTED_DATA_PATH,  # Use the correct extracted data path\n",
        "        'languages': ['hindi', 'english', 'chinese'],  # Languages to classify\n",
        "        'samples_per_lang': 15000,  # Maximum samples per language\n",
        "        'segment_length': 3,  # Audio segment length in seconds\n",
        "        'batch_size': 16,\n",
        "        'num_epochs': 30,\n",
        "        'learning_rate': 0.001,\n",
        "        'weight_decay': 1e-5,\n",
        "        'dropout_rate': 0.5,\n",
        "        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "        'save_dir': SAVE_DIR,\n",
        "        'apply_vad': True,  # Whether to apply voice activity detection\n",
        "        'resume_from': None,  # Path to checkpoint file for resuming training, None to start fresh\n",
        "    }\n",
        "\n",
        "    # Creating save directory\n",
        "    os.makedirs(config['save_dir'], exist_ok=True)\n",
        "\n",
        "    # Saving configuration\n",
        "    with open(os.path.join(config['save_dir'], 'config.json'), 'w') as f:\n",
        "        json.dump(config, f, indent=4)\n",
        "\n",
        "    # Setting-up logging\n",
        "    logger = setup_logging(config['save_dir'])\n",
        "    logger.info(f\"Starting Language Identification model training with config: {config}\")\n",
        "    logger.info(f\"Using device: {config['device']}\")\n",
        "\n",
        "    # Loading dataset\n",
        "    logger.info(\"Loading dataset...\")\n",
        "    full_dataset = LanguageAudioDataset(\n",
        "        root_dir=config['data_dir'],\n",
        "        languages=config['languages'],\n",
        "        max_samples_per_lang=config['samples_per_lang'],\n",
        "        segment_length=config['segment_length'],\n",
        "        apply_vad=config['apply_vad']\n",
        "    )\n",
        "\n",
        "    # Check if dataset is empty\n",
        "    if len(full_dataset) == 0:\n",
        "        logger.error(f\"Dataset is empty! Please check the path: {config['data_dir']}\")\n",
        "        logger.info(\"Available directories:\")\n",
        "        if os.path.exists(config['data_dir']):\n",
        "            logger.info(str(os.listdir(config['data_dir'])))\n",
        "        else:\n",
        "            logger.info(f\"Directory {config['data_dir']} does not exist!\")\n",
        "        return None, 0, None\n",
        "\n",
        "    logger.info(f\"Dataset loaded successfully with {len(full_dataset)} samples\")\n",
        "\n",
        "    # Splitting dataset\n",
        "    logger.info(\"Splitting dataset into train, validation, and test sets...\")\n",
        "    train_size = int(0.8 * len(full_dataset))\n",
        "    val_size = int(0.1 * len(full_dataset))\n",
        "    test_size = len(full_dataset) - train_size - val_size\n",
        "\n",
        "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
        "        full_dataset, [train_size, val_size, test_size]\n",
        "    )\n",
        "\n",
        "    logger.info(f\"Dataset split - Train: {len(train_dataset)}, \"\n",
        "               f\"Validation: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
        "\n",
        "    # Creating data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config['batch_size'],\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config['batch_size'],\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=config['batch_size'],\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Creating model\n",
        "    logger.info(\"Creating model...\")\n",
        "    model = LIDModel(num_languages=len(config['languages']))\n",
        "    model = model.to(config['device'])\n",
        "\n",
        "    # Use torchsummary to print model architecture\n",
        "    try:\n",
        "\n",
        "        # For spectrograms, something like (1, 80, 101) for (channels, height, width)\n",
        "        input_size = (1, 80, 101)  # Update with your actual spectrogram dimensions\n",
        "        logger.info(\"Generating model summary with torchsummary:\")\n",
        "        model_summary = summary(model, input_size, device=config['device'])\n",
        "\n",
        "        # Save the model summary to a file\n",
        "        summary_path = os.path.join(config['save_dir'], 'model_summary.txt')\n",
        "        with open(summary_path, 'w') as f:\n",
        "            # Redirect stdout to the file\n",
        "            import sys\n",
        "            original_stdout = sys.stdout\n",
        "            sys.stdout = f\n",
        "            summary(model, input_size, device=config['device'])\n",
        "            sys.stdout = original_stdout\n",
        "\n",
        "        logger.info(f\"Model summary saved to {summary_path}\")\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Failed to generate model summary with torchsummary: {e}\")\n",
        "        logger.info(f\"Model architecture:\\n{model}\")\n",
        "\n",
        "    # Defining loss function, optimizer, and scheduler\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=config['learning_rate'],\n",
        "        weight_decay=config['weight_decay']\n",
        "    )\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='min',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # Check if we need to resume training from a checkpoint\n",
        "    start_epoch = 0\n",
        "    resume_training_flag = False\n",
        "\n",
        "    if config['resume_from'] is not None and os.path.exists(config['resume_from']):\n",
        "        start_epoch, model, optimizer, scheduler = resume_training(\n",
        "            config['resume_from'], model, optimizer, scheduler, logger\n",
        "        )\n",
        "        resume_training_flag = True\n",
        "\n",
        "    # Training model\n",
        "    logger.info(\"Starting model training...\")\n",
        "    history = train_model(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        criterion=criterion,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        num_epochs=config['num_epochs'],\n",
        "        device=config['device'],\n",
        "        save_dir=config['save_dir'],\n",
        "        logger=logger,\n",
        "        start_epoch=start_epoch,\n",
        "        resume_training=resume_training_flag\n",
        "    )\n",
        "\n",
        "    # Evaluating model on test set\n",
        "    logger.info(\"Evaluating model on test set...\")\n",
        "    test_loss, test_acc, per_class_metrics = test_model(\n",
        "        model=model,\n",
        "        test_loader=test_loader,\n",
        "        criterion=criterion,\n",
        "        device=config['device'],\n",
        "        languages=config['languages'],\n",
        "        save_dir=config['save_dir'],\n",
        "        logger=logger\n",
        "    )\n",
        "\n",
        "\n",
        "    # Saving model in ONNX format for deployment\n",
        "    logger.info(\"Saving model in ONNX format...\")\n",
        "    save_model_as_onnx(model, os.path.join(config['save_dir'], 'lid_model.onnx'))\n",
        "\n",
        "    # Creating ZIP archive of the entire experiment\n",
        "    logger.info(\"Creating ZIP archive of experiment results...\")\n",
        "    zip_path = f\"{config['save_dir']}.zip\"\n",
        "    with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
        "        for root, dirs, files in os.walk(config['save_dir']):\n",
        "            for file in files:\n",
        "                zipf.write(\n",
        "                    os.path.join(root, file),\n",
        "                    os.path.relpath(os.path.join(root, file), os.path.join(config['save_dir'], '..'))\n",
        "                )\n",
        "\n",
        "    logger.info(f\"Experiment results archived to {zip_path}\")\n",
        "    logger.info(\"Language Identification model training and evaluation completed!\")\n",
        "\n",
        "    return history, test_acc, per_class_metrics\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-7ActUnpZoM9",
        "outputId": "eee6a080-47aa-41a9-9cf3-5facbaa0a82c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using audio data path: /content/language_data\n",
            "Checking for language directories:\n",
            "✓ Found hindi directory with 1 items\n",
            "✓ Found english directory with 1 items\n",
            "✓ Found chinese directory with 1 items\n",
            "Found 15000 files for hindi\n",
            "Found 15000 files for english\n",
            "Found 15000 files for chinese\n",
            "Total samples: 45000\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1             [-1, 101, 144]          11,664\n",
            "PositionalEncoding-2             [-1, 101, 144]               0\n",
            "           Dropout-3             [-1, 101, 144]               0\n",
            "            Linear-4             [-1, 101, 256]          37,120\n",
            "           Dropout-5             [-1, 101, 256]               0\n",
            "            Linear-6             [-1, 101, 144]          37,008\n",
            "       FeedForward-7             [-1, 101, 144]               0\n",
            "         LayerNorm-8             [-1, 101, 144]             288\n",
            "            Linear-9             [-1, 101, 144]          20,880\n",
            "           Linear-10             [-1, 101, 144]          20,880\n",
            "           Linear-11             [-1, 101, 144]          20,880\n",
            "          Dropout-12          [-1, 4, 101, 101]               0\n",
            "           Linear-13             [-1, 101, 144]          20,880\n",
            "MultiHeadedAttention-14             [-1, 101, 144]               0\n",
            "        LayerNorm-15             [-1, 101, 144]             288\n",
            "           Conv1d-16             [-1, 288, 101]          41,760\n",
            "           Conv1d-17             [-1, 144, 101]           4,608\n",
            "      BatchNorm1d-18             [-1, 144, 101]             288\n",
            "             SiLU-19             [-1, 144, 101]               0\n",
            "           Conv1d-20             [-1, 144, 101]          20,880\n",
            "          Dropout-21             [-1, 144, 101]               0\n",
            "       ConvModule-22             [-1, 101, 144]               0\n",
            "           Linear-23             [-1, 101, 256]          37,120\n",
            "          Dropout-24             [-1, 101, 256]               0\n",
            "           Linear-25             [-1, 101, 144]          37,008\n",
            "      FeedForward-26             [-1, 101, 144]               0\n",
            "        LayerNorm-27             [-1, 101, 144]             288\n",
            "   ConformerBlock-28             [-1, 101, 144]               0\n",
            "           Linear-29             [-1, 101, 256]          37,120\n",
            "          Dropout-30             [-1, 101, 256]               0\n",
            "           Linear-31             [-1, 101, 144]          37,008\n",
            "      FeedForward-32             [-1, 101, 144]               0\n",
            "        LayerNorm-33             [-1, 101, 144]             288\n",
            "           Linear-34             [-1, 101, 144]          20,880\n",
            "           Linear-35             [-1, 101, 144]          20,880\n",
            "           Linear-36             [-1, 101, 144]          20,880\n",
            "          Dropout-37          [-1, 4, 101, 101]               0\n",
            "           Linear-38             [-1, 101, 144]          20,880\n",
            "MultiHeadedAttention-39             [-1, 101, 144]               0\n",
            "        LayerNorm-40             [-1, 101, 144]             288\n",
            "           Conv1d-41             [-1, 288, 101]          41,760\n",
            "           Conv1d-42             [-1, 144, 101]           4,608\n",
            "      BatchNorm1d-43             [-1, 144, 101]             288\n",
            "             SiLU-44             [-1, 144, 101]               0\n",
            "           Conv1d-45             [-1, 144, 101]          20,880\n",
            "          Dropout-46             [-1, 144, 101]               0\n",
            "       ConvModule-47             [-1, 101, 144]               0\n",
            "           Linear-48             [-1, 101, 256]          37,120\n",
            "          Dropout-49             [-1, 101, 256]               0\n",
            "           Linear-50             [-1, 101, 144]          37,008\n",
            "      FeedForward-51             [-1, 101, 144]               0\n",
            "        LayerNorm-52             [-1, 101, 144]             288\n",
            "   ConformerBlock-53             [-1, 101, 144]               0\n",
            "           Linear-54             [-1, 101, 256]          37,120\n",
            "          Dropout-55             [-1, 101, 256]               0\n",
            "           Linear-56             [-1, 101, 144]          37,008\n",
            "      FeedForward-57             [-1, 101, 144]               0\n",
            "        LayerNorm-58             [-1, 101, 144]             288\n",
            "           Linear-59             [-1, 101, 144]          20,880\n",
            "           Linear-60             [-1, 101, 144]          20,880\n",
            "           Linear-61             [-1, 101, 144]          20,880\n",
            "          Dropout-62          [-1, 4, 101, 101]               0\n",
            "           Linear-63             [-1, 101, 144]          20,880\n",
            "MultiHeadedAttention-64             [-1, 101, 144]               0\n",
            "        LayerNorm-65             [-1, 101, 144]             288\n",
            "           Conv1d-66             [-1, 288, 101]          41,760\n",
            "           Conv1d-67             [-1, 144, 101]           4,608\n",
            "      BatchNorm1d-68             [-1, 144, 101]             288\n",
            "             SiLU-69             [-1, 144, 101]               0\n",
            "           Conv1d-70             [-1, 144, 101]          20,880\n",
            "          Dropout-71             [-1, 144, 101]               0\n",
            "       ConvModule-72             [-1, 101, 144]               0\n",
            "           Linear-73             [-1, 101, 256]          37,120\n",
            "          Dropout-74             [-1, 101, 256]               0\n",
            "           Linear-75             [-1, 101, 144]          37,008\n",
            "      FeedForward-76             [-1, 101, 144]               0\n",
            "        LayerNorm-77             [-1, 101, 144]             288\n",
            "   ConformerBlock-78             [-1, 101, 144]               0\n",
            "           Linear-79             [-1, 101, 256]          37,120\n",
            "          Dropout-80             [-1, 101, 256]               0\n",
            "           Linear-81             [-1, 101, 144]          37,008\n",
            "      FeedForward-82             [-1, 101, 144]               0\n",
            "        LayerNorm-83             [-1, 101, 144]             288\n",
            "           Linear-84             [-1, 101, 144]          20,880\n",
            "           Linear-85             [-1, 101, 144]          20,880\n",
            "           Linear-86             [-1, 101, 144]          20,880\n",
            "          Dropout-87          [-1, 4, 101, 101]               0\n",
            "           Linear-88             [-1, 101, 144]          20,880\n",
            "MultiHeadedAttention-89             [-1, 101, 144]               0\n",
            "        LayerNorm-90             [-1, 101, 144]             288\n",
            "           Conv1d-91             [-1, 288, 101]          41,760\n",
            "           Conv1d-92             [-1, 144, 101]           4,608\n",
            "      BatchNorm1d-93             [-1, 144, 101]             288\n",
            "             SiLU-94             [-1, 144, 101]               0\n",
            "           Conv1d-95             [-1, 144, 101]          20,880\n",
            "          Dropout-96             [-1, 144, 101]               0\n",
            "       ConvModule-97             [-1, 101, 144]               0\n",
            "           Linear-98             [-1, 101, 256]          37,120\n",
            "          Dropout-99             [-1, 101, 256]               0\n",
            "          Linear-100             [-1, 101, 144]          37,008\n",
            "     FeedForward-101             [-1, 101, 144]               0\n",
            "       LayerNorm-102             [-1, 101, 144]             288\n",
            "  ConformerBlock-103             [-1, 101, 144]               0\n",
            "          Linear-104             [-1, 101, 256]          37,120\n",
            "         Dropout-105             [-1, 101, 256]               0\n",
            "          Linear-106             [-1, 101, 144]          37,008\n",
            "     FeedForward-107             [-1, 101, 144]               0\n",
            "       LayerNorm-108             [-1, 101, 144]             288\n",
            "          Linear-109             [-1, 101, 144]          20,880\n",
            "          Linear-110             [-1, 101, 144]          20,880\n",
            "          Linear-111             [-1, 101, 144]          20,880\n",
            "         Dropout-112          [-1, 4, 101, 101]               0\n",
            "          Linear-113             [-1, 101, 144]          20,880\n",
            "MultiHeadedAttention-114             [-1, 101, 144]               0\n",
            "       LayerNorm-115             [-1, 101, 144]             288\n",
            "          Conv1d-116             [-1, 288, 101]          41,760\n",
            "          Conv1d-117             [-1, 144, 101]           4,608\n",
            "     BatchNorm1d-118             [-1, 144, 101]             288\n",
            "            SiLU-119             [-1, 144, 101]               0\n",
            "          Conv1d-120             [-1, 144, 101]          20,880\n",
            "         Dropout-121             [-1, 144, 101]               0\n",
            "      ConvModule-122             [-1, 101, 144]               0\n",
            "          Linear-123             [-1, 101, 256]          37,120\n",
            "         Dropout-124             [-1, 101, 256]               0\n",
            "          Linear-125             [-1, 101, 144]          37,008\n",
            "     FeedForward-126             [-1, 101, 144]               0\n",
            "       LayerNorm-127             [-1, 101, 144]             288\n",
            "  ConformerBlock-128             [-1, 101, 144]               0\n",
            "          Linear-129             [-1, 101, 256]          37,120\n",
            "         Dropout-130             [-1, 101, 256]               0\n",
            "          Linear-131             [-1, 101, 144]          37,008\n",
            "     FeedForward-132             [-1, 101, 144]               0\n",
            "       LayerNorm-133             [-1, 101, 144]             288\n",
            "          Linear-134             [-1, 101, 144]          20,880\n",
            "          Linear-135             [-1, 101, 144]          20,880\n",
            "          Linear-136             [-1, 101, 144]          20,880\n",
            "         Dropout-137          [-1, 4, 101, 101]               0\n",
            "          Linear-138             [-1, 101, 144]          20,880\n",
            "MultiHeadedAttention-139             [-1, 101, 144]               0\n",
            "       LayerNorm-140             [-1, 101, 144]             288\n",
            "          Conv1d-141             [-1, 288, 101]          41,760\n",
            "          Conv1d-142             [-1, 144, 101]           4,608\n",
            "     BatchNorm1d-143             [-1, 144, 101]             288\n",
            "            SiLU-144             [-1, 144, 101]               0\n",
            "          Conv1d-145             [-1, 144, 101]          20,880\n",
            "         Dropout-146             [-1, 144, 101]               0\n",
            "      ConvModule-147             [-1, 101, 144]               0\n",
            "          Linear-148             [-1, 101, 256]          37,120\n",
            "         Dropout-149             [-1, 101, 256]               0\n",
            "          Linear-150             [-1, 101, 144]          37,008\n",
            "     FeedForward-151             [-1, 101, 144]               0\n",
            "       LayerNorm-152             [-1, 101, 144]             288\n",
            "  ConformerBlock-153             [-1, 101, 144]               0\n",
            "          Linear-154                  [-1, 144]          20,880\n",
            "            ReLU-155                  [-1, 144]               0\n",
            "         Dropout-156                  [-1, 144]               0\n",
            "          Linear-157                    [-1, 3]             435\n",
            "       Conformer-158                    [-1, 3]               0\n",
            "================================================================\n",
            "Total params: 1,834,035\n",
            "Trainable params: 1,834,035\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.03\n",
            "Forward/backward pass size (MB): 20.92\n",
            "Params size (MB): 7.00\n",
            "Estimated Total Size (MB): 27.95\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Train Epoch 1: 100%|██████████| 2250/2250 [04:40<00:00,  8.01it/s, loss=0.4884, acc=67.47%]\n",
            "Val Epoch 1: 100%|██████████| 282/282 [00:30<00:00,  9.19it/s, loss=0.7795, acc=77.69%]\n",
            "Train Epoch 2: 100%|██████████| 2250/2250 [04:14<00:00,  8.85it/s, loss=0.7226, acc=79.76%]\n",
            "Val Epoch 2: 100%|██████████| 282/282 [00:30<00:00,  9.29it/s, loss=0.3900, acc=69.47%]\n",
            "Train Epoch 3: 100%|██████████| 2250/2250 [04:12<00:00,  8.91it/s, loss=0.5515, acc=83.34%]\n",
            "Val Epoch 3: 100%|██████████| 282/282 [00:29<00:00,  9.50it/s, loss=0.2319, acc=82.67%]\n",
            "Train Epoch 4: 100%|██████████| 2250/2250 [04:14<00:00,  8.85it/s, loss=0.2112, acc=85.54%]\n",
            "Val Epoch 4: 100%|██████████| 282/282 [00:31<00:00,  8.95it/s, loss=0.0213, acc=87.64%]\n",
            "Train Epoch 5: 100%|██████████| 2250/2250 [04:18<00:00,  8.72it/s, loss=0.6636, acc=87.29%]\n",
            "Val Epoch 5: 100%|██████████| 282/282 [00:30<00:00,  9.24it/s, loss=0.3380, acc=89.64%]\n",
            "Train Epoch 6: 100%|██████████| 2250/2250 [04:15<00:00,  8.81it/s, loss=0.9272, acc=88.52%]\n",
            "Val Epoch 6: 100%|██████████| 282/282 [00:30<00:00,  9.23it/s, loss=0.0379, acc=88.78%]\n",
            "Train Epoch 7: 100%|██████████| 2250/2250 [04:12<00:00,  8.91it/s, loss=0.0696, acc=89.57%]\n",
            "Val Epoch 7: 100%|██████████| 282/282 [00:30<00:00,  9.16it/s, loss=0.1150, acc=89.93%]\n",
            "Train Epoch 8: 100%|██████████| 2250/2250 [04:12<00:00,  8.92it/s, loss=0.0957, acc=90.11%]\n",
            "Val Epoch 8: 100%|██████████| 282/282 [00:29<00:00,  9.45it/s, loss=0.2276, acc=90.02%]\n",
            "Train Epoch 9: 100%|██████████| 2250/2250 [04:09<00:00,  9.00it/s, loss=0.4621, acc=91.05%]\n",
            "Val Epoch 9: 100%|██████████| 282/282 [00:29<00:00,  9.59it/s, loss=0.1001, acc=92.00%]\n",
            "Train Epoch 10: 100%|██████████| 2250/2250 [04:11<00:00,  8.93it/s, loss=0.1396, acc=91.67%]\n",
            "Val Epoch 10: 100%|██████████| 282/282 [00:30<00:00,  9.19it/s, loss=1.3565, acc=90.36%]\n",
            "Train Epoch 11: 100%|██████████| 2250/2250 [04:08<00:00,  9.06it/s, loss=0.0944, acc=92.10%]\n",
            "Val Epoch 11: 100%|██████████| 282/282 [00:29<00:00,  9.40it/s, loss=0.7987, acc=91.62%]\n",
            "Train Epoch 12: 100%|██████████| 2250/2250 [04:10<00:00,  8.98it/s, loss=0.2655, acc=92.33%]\n",
            "Val Epoch 12: 100%|██████████| 282/282 [00:29<00:00,  9.41it/s, loss=0.0532, acc=91.07%]\n",
            "Train Epoch 13: 100%|██████████| 2250/2250 [04:09<00:00,  9.00it/s, loss=0.2249, acc=92.61%]\n",
            "Val Epoch 13: 100%|██████████| 282/282 [00:30<00:00,  9.16it/s, loss=0.6141, acc=91.60%]\n",
            "Train Epoch 14: 100%|██████████| 2250/2250 [04:15<00:00,  8.81it/s, loss=0.2795, acc=94.44%]\n",
            "Val Epoch 14: 100%|██████████| 282/282 [00:31<00:00,  9.07it/s, loss=0.0935, acc=92.98%]\n",
            "Train Epoch 15: 100%|██████████| 2250/2250 [04:12<00:00,  8.89it/s, loss=0.0112, acc=94.75%]\n",
            "Val Epoch 15: 100%|██████████| 282/282 [00:30<00:00,  9.15it/s, loss=0.0070, acc=93.47%]\n",
            "Train Epoch 16: 100%|██████████| 2250/2250 [04:11<00:00,  8.96it/s, loss=0.0661, acc=94.96%]\n",
            "Val Epoch 16: 100%|██████████| 282/282 [00:33<00:00,  8.32it/s, loss=1.1410, acc=94.31%]\n",
            "Train Epoch 17: 100%|██████████| 2250/2250 [04:13<00:00,  8.88it/s, loss=0.1401, acc=95.34%]\n",
            "Val Epoch 17: 100%|██████████| 282/282 [00:30<00:00,  9.17it/s, loss=0.0116, acc=92.16%]\n",
            "Train Epoch 18: 100%|██████████| 2250/2250 [04:15<00:00,  8.80it/s, loss=0.1499, acc=95.39%]\n",
            "Val Epoch 18: 100%|██████████| 282/282 [00:31<00:00,  8.90it/s, loss=0.0082, acc=93.53%]\n",
            "Train Epoch 19: 100%|██████████| 2250/2250 [04:13<00:00,  8.87it/s, loss=0.1610, acc=95.54%]\n",
            "Val Epoch 19: 100%|██████████| 282/282 [00:32<00:00,  8.62it/s, loss=0.0006, acc=93.89%]\n",
            "Train Epoch 20: 100%|██████████| 2250/2250 [04:26<00:00,  8.44it/s, loss=0.0101, acc=95.60%]\n",
            "Val Epoch 20: 100%|██████████| 282/282 [00:34<00:00,  8.20it/s, loss=0.0067, acc=94.42%]\n",
            "Train Epoch 21: 100%|██████████| 2250/2250 [04:30<00:00,  8.32it/s, loss=0.0313, acc=95.89%]\n",
            "Val Epoch 21: 100%|██████████| 282/282 [00:32<00:00,  8.66it/s, loss=0.0063, acc=94.18%]\n",
            "Train Epoch 22: 100%|██████████| 2250/2250 [04:18<00:00,  8.69it/s, loss=0.1485, acc=95.78%]\n",
            "Val Epoch 22: 100%|██████████| 282/282 [00:30<00:00,  9.22it/s, loss=0.6695, acc=93.89%]\n",
            "Train Epoch 23: 100%|██████████| 2250/2250 [04:16<00:00,  8.79it/s, loss=0.0080, acc=95.94%]\n",
            "Val Epoch 23: 100%|██████████| 282/282 [00:31<00:00,  9.04it/s, loss=0.0004, acc=94.29%]\n",
            "Train Epoch 24: 100%|██████████| 2250/2250 [04:14<00:00,  8.84it/s, loss=0.1375, acc=96.02%]\n",
            "Val Epoch 24: 100%|██████████| 282/282 [00:31<00:00,  9.05it/s, loss=0.0183, acc=93.33%]\n",
            "Train Epoch 25: 100%|██████████| 2250/2250 [04:14<00:00,  8.85it/s, loss=0.1465, acc=96.74%]\n",
            "Val Epoch 25: 100%|██████████| 282/282 [00:30<00:00,  9.12it/s, loss=0.0005, acc=95.07%]\n",
            "Train Epoch 26: 100%|██████████| 2250/2250 [04:17<00:00,  8.75it/s, loss=0.0776, acc=96.86%]\n",
            "Val Epoch 26: 100%|██████████| 282/282 [00:31<00:00,  8.95it/s, loss=0.0019, acc=94.49%]\n",
            "Train Epoch 27: 100%|██████████| 2250/2250 [04:16<00:00,  8.76it/s, loss=0.0475, acc=96.94%]\n",
            "Val Epoch 27: 100%|██████████| 282/282 [00:30<00:00,  9.21it/s, loss=0.0002, acc=94.47%]\n",
            "Train Epoch 28: 100%|██████████| 2250/2250 [04:14<00:00,  8.86it/s, loss=0.0004, acc=97.09%]\n",
            "Val Epoch 28: 100%|██████████| 282/282 [00:32<00:00,  8.75it/s, loss=0.0000, acc=94.49%]\n",
            "Train Epoch 29: 100%|██████████| 2250/2250 [04:13<00:00,  8.89it/s, loss=0.1225, acc=97.13%]\n",
            "Val Epoch 29: 100%|██████████| 282/282 [00:31<00:00,  9.06it/s, loss=0.0001, acc=94.24%]\n",
            "Train Epoch 30: 100%|██████████| 2250/2250 [04:17<00:00,  8.74it/s, loss=0.1431, acc=97.40%]\n",
            "Val Epoch 30: 100%|██████████| 282/282 [00:31<00:00,  8.97it/s, loss=0.7434, acc=95.18%]\n",
            "Testing: 100%|██████████| 282/282 [00:31<00:00,  9.05it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'plot_training_history' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-62a350404330>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-62a350404330>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;31m# Plotting training history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0mplot_training_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'save_dir'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;31m# Saving model in ONNX format for deployment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plot_training_history' is not defined"
          ]
        }
      ]
    }
  ]
}